import tflite_runtime.interpreter as tflite
import numpy as np
import cv2
import time
import multiprocessing as mp
import warnings

warnings.simplefilter("ignore", UserWarning)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
model_path = "/home/willtek/Documents/mini_project/weights/MobileNetV3A_RP_Hard_LossAll.tflite"

# í´ë˜ìŠ¤ ë§¤í•‘
ansToText = {0: 'rock', 1: 'scissors', 2: 'paper'}
colorList = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]

# ì¹´ë©”ë¼ ì„¤ì •
WIDTH, HEIGHT = 400, 300  # ì›í•˜ëŠ” í•´ìƒë„ë¡œ ì„¤ì •

# âœ… í”„ë ˆì„ ìº¡ì²˜ í”„ë¡œì„¸ìŠ¤
def capture_frames(frame_queue):
    cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
    cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*"MJPG"))  # MJPEG í¬ë§· ì‚¬ìš©
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, 30)  # 30FPS ì„¤ì •

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        frame = cv2.flip(frame, 1)  # ì¢Œìš° ë°˜ì „

        # ìµœì‹  í”„ë ˆì„ ìœ ì§€ (íê°€ ê°€ë“ ì°¨ë©´ ì´ì „ í”„ë ˆì„ ì‚­ì œ)
        if frame_queue.full():
            frame_queue.get()
        frame_queue.put(frame)

    cap.release()

# âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_frame(frame):
    """ì¹´ë©”ë¼ í”„ë ˆì„ì„ ëª¨ë¸ ì…ë ¥ í¬ê¸°ì— ë§ê²Œ ë³€í™˜"""
    frame = cv2.resize(frame, (WIDTH, HEIGHT), interpolation=cv2.INTER_AREA)  # ë¹ ë¥¸ ë¦¬ì‚¬ì´ì§•
    frame = frame[..., ::-1]  # BGR â†’ RGB ë³€í™˜ (cvtColor ëŒ€ì‹  NumPy ì‚¬ìš©)
    frame = (frame.astype(np.float32) / 255.0).transpose(2, 0, 1)  # (H, W, C) â†’ (C, H, W)
    return np.expand_dims(frame, axis=0)  # (1, C, H, W) í˜•íƒœë¡œ ë³€í™˜

# âœ… Softmax í•¨ìˆ˜
def softmax(x):
    exp_x = np.exp(x - np.max(x))  # ì˜¤ë²„í”Œë¡œ ë°©ì§€
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

# âœ… ì¶”ë¡  í”„ë¡œì„¸ìŠ¤
def inference(frame_queue, result_queue):
    try:
        # GPU Delegate ì„¤ì •
        interpreter = tflite.Interpreter(
            model_path=model_path,
            experimental_delegates=[tflite.load_delegate('libtensorflowlite_delegate_gpu.so')]
        )
        print("âœ… GPU ê°€ì† í™œì„±í™”ë¨")
    except:
        interpreter = tflite.Interpreter(model_path=model_path, num_threads=4)
        print("ğŸ–¥ï¸ CPUë¡œ ì‹¤í–‰")

    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    input_dtype = input_details[0]['dtype']

    start_time = time.time()
    
    while True:
        if not frame_queue.empty():
            frame = frame_queue.get()

            # FPS ê³„ì‚°
            cur_time = time.time()
            fps = 1 / (cur_time - start_time)
            # print(f"Latency: {(cur_time - start_time):.2f}")
            print(f"Queue size: {frame_queue.qsize()}")
            start_time = cur_time

            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            input_data = preprocess_frame(frame)

            # ëª¨ë¸ ì…ë ¥ ì„¤ì • ë° ì‹¤í–‰
            interpreter.set_tensor(input_details[0]['index'], input_data.astype(input_dtype))
            interpreter.invoke()

            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            output_data = interpreter.get_tensor(output_details[0]['index'])[0]
            probabilities = softmax(output_data)  # Softmax ì ìš©
            ans = np.argmax(probabilities)
            confidence = probabilities[ans]

            # 50% ë¯¸ë§Œì´ë©´ invalid ì²˜ë¦¬
            if confidence < 0.5:
                text, color = "invalid", (0, 0, 0)  # ê²€ì€ìƒ‰
            else:
                text, color = ansToText[ans], colorList[ans]

            # ê²°ê³¼ ì „ì†¡
            result_queue.put((frame, text, color, fps))

# âœ… ë©”ì¸ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤
def main():
    mp.set_start_method("spawn", force=True)  # ë©€í‹°í”„ë¡œì„¸ì‹± ì´ˆê¸°í™”

    # ê³µìœ  í ìƒì„±
    frame_queue = mp.Queue(maxsize=5)  # ìµœì‹  í”„ë ˆì„ë§Œ ìœ ì§€
    result_queue = mp.Queue()

    # í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    capture_process = mp.Process(target=capture_frames, args=(frame_queue,))
    inference_process = mp.Process(target=inference, args=(frame_queue, result_queue))

    capture_process.start()
    inference_process.start()

    while True:
        if not result_queue.empty():
            frame, text, color, fps = result_queue.get()

            # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥
            cv2.putText(frame, text, (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
            cv2.putText(frame, f'FPS: {fps:.1f}', (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)

            # í™”ë©´ ì¶œë ¥
            cv2.imshow("Webcam", frame)

        # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
    capture_process.terminate()
    inference_process.terminate()
    capture_process.join()
    inference_process.join()

    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
